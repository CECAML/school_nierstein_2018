{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not forget to swapoff -a\n",
    "import numpy as np       # linear algebra\n",
    "import pylab as pl       # plots\n",
    "import tensorflow as tf  # now we are actually using it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((60000, 28, 28), (60000,))\n",
      "((10000, 28, 28), (10000,))\n"
     ]
    }
   ],
   "source": [
    "# choose wisely\n",
    "(train, label_train), (test, label_test) = tf.keras.datasets.mnist.load_data()\n",
    "#(train, label_train), (test, label_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# make sure your data is floating point\n",
    "test = test.astype(np.float32)\n",
    "train = train.astype(np.float32)\n",
    "\n",
    "# print shapes\n",
    "print(train.shape, label_train.shape)\n",
    "print(test.shape, label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((60000, 28, 28), (60000,))\n",
      "((10000, 28, 28), (10000,))\n"
     ]
    }
   ],
   "source": [
    "# here we can subsample in the data set\n",
    "num_test, num_train = 10000, 60000\n",
    "test, label_test = test[:num_test], label_test[:num_test]\n",
    "train, label_train = train[:num_train], label_train[:num_train]\n",
    "\n",
    "# print shapes\n",
    "print(train.shape, label_train.shape)\n",
    "print(test.shape, label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now define the session graph\n",
    "X_tf = tf.placeholder(shape=[None, 28, 28, 1], dtype=tf.float32) # for each image we have 784 pixels\n",
    "Y_tf = tf.placeholder(shape=[None],            dtype=tf.int64)   # for each image we have one scalar label\n",
    "\n",
    "# make train and test compatible\n",
    "train = np.expand_dims(train, 3)\n",
    "test  = np.expand_dims(test,  3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.layers as tfl\n",
    "\n",
    "def prelu(net):\n",
    "    alpha = tf.Variable(0.0, dtype=net.dtype)\n",
    "    return tf.nn.leaky_relu(net, alpha)\n",
    "\n",
    "def residual_conv_block(net, num_filters, kernel_size, stride, is_training=True):\n",
    "    \n",
    "    # let us cache the input tensor and downsample it\n",
    "    inp = tfl.avg_pool2d(net, kernel_size, stride, padding=\"SAME\")\n",
    "    \n",
    "    # now convolve with stride (potential downsampling)\n",
    "    net = tfl.conv2d(net, num_filters, kernel_size, stride, activation_fn=tf.identity, padding=\"SAME\")\n",
    "    \n",
    "    # normalize the output\n",
    "    net = tfl.batch_norm(net, is_training=is_training, activation_fn=tf.identity)\n",
    "    \n",
    "    # now convolve again but do not downsample\n",
    "    net = tfl.conv2d(net, num_filters, kernel_size, stride=1, activation_fn=tf.identity, padding=\"SAME\")\n",
    "\n",
    "    return prelu(tf.concat((net, inp), axis=-1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = tf.identity(X_tf)\n",
    "net = residual_conv_block(net,  16, 3, 2)\n",
    "net = residual_conv_block(net,  32, 3, 2)\n",
    "net = residual_conv_block(net,  64, 3, 2)\n",
    "net = residual_conv_block(net, 128, 3, 2)\n",
    "\n",
    "# make it completely translation invariant\n",
    "net = tf.reduce_mean(net, axis=(1, 2))\n",
    "net = tfl.fully_connected(net, 10, activation_fn=tf.identity)\n",
    "\n",
    "# the same loss like in softmax regression just now in short\n",
    "loss_tf = tf.losses.sparse_softmax_cross_entropy(labels=Y_tf, logits=net)\n",
    "\n",
    "# let us define the non-differentiable accuracy as metric\n",
    "correctly_predicted = tf.equal(Y_tf, tf.argmax(net, axis=1))\n",
    "metric_tf = tf.reduce_mean(tf.cast(correctly_predicted, tf.float32))\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = tf.train.AdamOptimizer(1E-3)\n",
    "step_tf = optimizer.minimize(loss_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 33/512 [00:06<01:34,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train loss and metric:', 0.5179426, 0.8515625)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 65/512 [00:12<01:26,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train loss and metric:', 0.26763347, 0.921875)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 96/512 [00:19<01:25,  4.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train loss and metric:', 0.10120472, 0.984375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 129/512 [00:27<01:21,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train loss and metric:', 0.07530877, 0.9765625)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 160/512 [00:33<01:14,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train loss and metric:', 0.12366684, 0.953125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 193/512 [00:41<01:08,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train loss and metric:', 0.09836517, 0.96875)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 224/512 [00:48<01:01,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train loss and metric:', 0.037775334, 0.984375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 256/512 [00:59<00:59,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train loss and metric:', 0.076444976, 0.9765625)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 288/512 [01:08<00:53,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train loss and metric:', 0.09011425, 0.97265625)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 320/512 [01:15<00:45,  4.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train loss and metric:', 0.04790362, 0.984375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 352/512 [01:22<00:37,  4.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train loss and metric:', 0.052710272, 0.984375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 384/512 [01:30<00:30,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train loss and metric:', 0.04114881, 0.984375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 417/512 [01:36<00:22,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train loss and metric:', 0.01993398, 0.99609375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 449/512 [01:43<00:14,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train loss and metric:', 0.040300496, 0.98828125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 480/512 [01:51<00:07,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train loss and metric:', 0.024529546, 0.99609375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [01:58<00:00,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train loss and metric:', 0.019960929, 0.9921875)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test loss and metric:', 0.048200987, 0.9847)\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    num_iterations, print_every, batch_size = 2**9, 2**5, 2**8\n",
    "    for iteration in tqdm.tqdm(range(num_iterations)):\n",
    "        \n",
    "        indices = np.random.choice(train.shape[0], batch_size, replace=False)\n",
    "        X, Y = train[indices], label_train[indices]\n",
    "        \n",
    "        sess.run(step_tf, feed_dict={X_tf: X, Y_tf: Y})\n",
    "        \n",
    "        if iteration % print_every == print_every-1:\n",
    "            loss, metric = sess.run([loss_tf, metric_tf], feed_dict={X_tf: X, Y_tf: Y})\n",
    "            print(\"train loss and metric:\",loss, metric)\n",
    "\n",
    "\n",
    "    loss, metric = sess.run([loss_tf, metric_tf], feed_dict={X_tf: test, Y_tf: label_test})\n",
    "    print(\"test loss and metric:\", loss, metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
